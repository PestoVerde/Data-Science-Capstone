---
author: "Sergey Cherkasov"
date: "11 Dec 2015"
output: html_document
---
### Data Science Capstone - Natural Language Processing
# Milestone Report

In this capstone we will work on understanding and building predictive text models for smart keyborad like those produced by [SwiftKey](https://swiftkey.com/en). To do so we are going to use three set of English texts of three different type - blogs, news and twits. The data is from a corpus called [HC Corpora](www.corpora.heliohost.org).

This paper is the first preliminary report of our work. It explains our exploratory analysis and our plans for the creating algorithm and application.

To keep the paper compact, we do not echo all the code. To see complete code, please visit [github page](github.com/PestoVerde/Data-Science-Capstone).

###Data acquisition and cleaning

First of all we upload if necessary the set of text files (code not shown). Then we create main structure for managing documents in tm package (so-called Corpus), representing a collection of text documents. But first of all let's plug in some useful libraries.

``` {r echo = TRUE}
# Let us turn on some libraries.
libs <- c("rJava", "stringi", "NLP", "tm", "filehash", "RWeka")
sapply(libs, library, character.only = T, logical.return = T)
```

``` {r echo = FALSE}
# Here we check is there folder with data already exits in working directory. 
# If not we upload it into working directory

Windows <- FALSE

if(!("final" %in% dir())){
    if(Sys.info()["sysname"]=="Windows") Windows <- TRUE
    url.data.file <- "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
    temp.zip.file.name <- "temp.zip"
    if(Windows) download.file(url.data.file, temp.zip.file.name) else
        download.file(url.data.file, temp.zip.file.name, method="curl")
    unzip(temp.zip.file.name)
    file.remove(temp.zip.file.name)
}
```

``` {r echo = TRUE}
# Here we create a Corpus.
crp <- VCorpus(DirSource("final/en_US/", encoding = "UTF-8"), 
               readerControl = list(language = "en_US", load = T))
```

Let us have a quick initial look on what we get.

``` {r echo = F}
q.summ <- data.frame(matrix(0, ncol = 4, nrow = 3))
q.summ[, 1] <- unlist(sapply(paste("final/en_US/", list.files("final/en_US/"), sep = ""), file.info)[1,])/1024/1024
q.summ[, 2] <- sapply(names(crp), function(x) length(crp[[x]][[1]])/1000)
q.summ[, 3] <- sapply(names(crp), 
                      function(x) sum(stri_count(crp[[x]][[1]],regex="\\S+"))/1000000)
q.summ[, 4] <- sapply(names(crp), function(x) sum(nchar(crp[[x]][[1]]))/1000000)
q.summ <- round(q.summ, 0)
names(q.summ) <- c("File size (Mb)", "Lines (thds)", "Word (mln)", "Characters (mln)")
row.names(q.summ) <- c("Blogs", "News", "Twits")
q.summ
```

It looks like we have enough data for prediction. But before exploratory analysis and creation a model we have to clean data. The are several standard steps in linguistic data mining, which are putting all letters to lower case, removing extra spaces, punctuation and other non-alphabetic symbols, removing profanity words and so-called "stop-word", working with roots of words (tokenization, stemming or lemmatization) and creating n-grams. Here are some considerations regarding some steps in the light of our task.

1. Since we have texts from twitter we have to remove all words in the form of **@user** and **#hashtag** before removing all the non-alphabetic symbols, otherwise we will have many senseless words.
2. Should we remove apostrophes? They indicate possessive mode in English, can they change meaning of the frase? That is the question we can answer at the stage of building models.
3. Should we remove dashes? They create new composite word, like **Alabama-based** which obviously have to be predicted separately. However since we are going to use 2- and 3-grams, it looks like dashes can be removed. 
4. It looks like we also have to mark boundaries of sentences, they seem as inportant information for prediction. Again, we can find an answer when we will build a model.
5. For tokenized text words "car" and "cars" are different. For stemmed text words "car" and "cars" are the same. For lemmatisated text words "car", "cars", "automobile" and "automobiles" are the same. At first glance we have to tokenize our text for better prediction. However this is the topic of following researches which are beyond exploratory analysis.
6. We also want to remove all profanity words because we do not want to predict them. However "stop word" like **the**, **a**, **for**, **nor**, **but** etc. we would like to leave because we are going to predict them too.

``` {r echo = F}
crp_twitter <- function(x){ #Function removes all #hashtag and @users in a line
  x <- gsub("[@#]\\S+\\w", "", x)
  x
}

profanity <- readLines("profanity.txt") #List of profanity words from file

crp <- tm_map(crp, content_transformer(crp_twitter)) #Line removes all #hashtags and @users
crp <- tm_map(crp, content_transformer(tolower)) #Line change all capital letter to lower case
crp <- tm_map(crp, stripWhitespace) #Line removes extra white space
crp <- tm_map(crp, removePunctuation) #Line removes all punctuation
crp <- tm_map(crp, removeNumbers) #Line removes all numbers
crp <- tm_map(crp, removeWords, profanity) #Line removes profanity words
```

Since the Corpus is huge we sample it by taking 10.000 lines from each of three documents (code not shown). Otherwise all further manipulations with take considerable time. (code not shown).
``` {r echo = f}
# SAMPLING
writeCorpus(crp, "cleaned_all")
set.seed(8310)
blogs <- readLines("cleaned_all\en_US.blogs.txt.txt")
blogs <- sample(blogs, 10000)
news <- readLines("cleaned_all\en_US.news.txt.txt")
news <- sample(news, 10000)
twitter <- readLines("cleaned_all\en_US.twitter.txt.txt")
twitter <- sample(twitter, 10000)
writeLines(twitter, "cleaned_samples\twitter.txt")
writeLines(blogs, "cleaned_samples\blogs.txt")
writeLines(news, "cleaned_samples\news.txt")
crp <- VCorpus(DirSource("cleaned_samples", encoding = "UTF-8"), readerControl = list(language = "en_US", load = T))

```

And now we tokenize the corpus and build n-grams for n=2 and n=3. (All function we use could be found in Appendix below)
``` {r echo = F}
OnegramTokenizer <-
  function(x)
    unlist(lapply(ngrams(words(x), 1), paste, collapse = " "), use.names = FALSE)

BigramTokenizer <-
  function(x)
    unlist(lapply(ngrams(words(x), 2), paste, collapse = " "), use.names = FALSE)

TrigramTokenizer <-
  function(x)
    unlist(lapply(ngrams(words(x), 3), paste, collapse = " "), use.names = FALSE)
```

``` {r echo = T}
one.matrix <- TermDocumentMatrix(crp, control = list(tokenize = OnegramTokenizer))
bi.matrix <- TermDocumentMatrix(crp, control = list(tokenize = BigramTokenizer))
tri.matrix <- TermDocumentMatrix(crp, control = list(tokenize = TrigramTokenizer))
```

###Exploratory Analysis
``` {r echo = T}
plotting.ngrams <- function(a, n=6){
  a <- as.data.frame(as.matrix(a))
  a$TOTAL <- rowSums(a)
  a <- a[order(-a$TOTAL),]
  a <- a[1:n,]
  a <- data.frame(row.names(a), a$TOTAL)
  names(a) <- c("Words", "Amount")
  p <-  ggplot(a, aes(x=reorder(Words, Amount), Amount))
  #p <- qplot(x=Words, Amount, data=a)
  #p <- ggplot(a, aes(x = Words, y = Amount))
  p <- p + geom_bar(stat = "identity", color = "lightgreen", fill = "lightgreen") + 
           geom_text(aes(label = Amount), size = 3, position = "stack") +
           xlab("Words") + 
           ylab("Amount") + 
           ggtitle(paste(n, "most frequent n-grams")) + 
           coord_flip()
  p
}
```

``` {r echo = T}
plotting.ngrams(one.matrix, 20)
plotting.ngrams(bi.matrix, 20)
plotting.ngrams(tri.matrix, 20)
```

###Appendix - Functions
``` {r echo = T}
OnegramTokenizer <-
  function(x)
    unlist(lapply(ngrams(words(x), 1), paste, collapse = " "), use.names = FALSE)

BigramTokenizer <-
  function(x)
    unlist(lapply(ngrams(words(x), 2), paste, collapse = " "), use.names = FALSE)

TrigramTokenizer <-
  function(x)
    unlist(lapply(ngrams(words(x), 3), paste, collapse = " "), use.names = FALSE)

plotting.ngrams <- function(a, n=6){
  a <- as.data.frame(as.matrix(a))
  a$TOTAL <- rowSums(a)
  a <- a[order(-a$TOTAL),]
  a <- a[1:n,]
  a <- data.frame(row.names(a), a$TOTAL)
  names(a) <- c("Words", "Amount")
  p <-  ggplot(a, aes(x=reorder(Words, Amount), Amount))
  #p <- qplot(x=Words, Amount, data=a)
  #p <- ggplot(a, aes(x = Words, y = Amount))
  p <- p + geom_bar(stat = "identity", color = "lightgreen", fill = "lightgreen") + 
           geom_text(aes(label = Amount), size = 3, position = "stack") +
           xlab("Words") + 
           ylab("Amount") + 
           ggtitle(paste(n, "most frequent n-grams")) + 
           coord_flip()
  p
}

```
