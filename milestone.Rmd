---
author: "Sergey Cherkasov"
date: "11 Dec 2015"
output: html_document
---
## Data Science Capstone - Natural Language Processing
# Milestone Report

In this capstone we will work on understanding and building predictive text models for smart keyborad like those produced by [SwiftKey](https://swiftkey.com/en). To do so we are going to use three set of English texts of three different type - blogs, news and twits. The data is from a corpus called [HC Corpora](www.corpora.heliohost.org).

This paper is the first preliminary report of our work. It explains our exploratory analysis and our plans for the creating algorithm and application.

To keep the paper compact, we do not echo all the code. To see complete code, please visit [github page](github.com/PestoVerde/Data-Science-Capstone). All function we use could be found in Appendix below.

##Data acquisition and cleaning

First of all we upload if necessary the set of text files (code not shown). Then we create main structure for managing documents in tm package (so-called Corpus), representing a collection of text documents. But first of all let's plug in some useful libraries.

``` {r echo = TRUE}
# Let us turn on some libraries.
libs <- c("ggplot2", "rJava", "stringi", "NLP", "openNLP", "tm", "filehash", "RWeka")
sapply(libs, library, character.only = T, logical.return = T, 
       quietly = T, warn.conflicts = F)
```

``` {r echo = FALSE}
# Here we check is there folder with data already exits in working directory. 
# If not we upload it into working directory

Windows <- FALSE

if(!("final" %in% dir())){
    if(Sys.info()["sysname"]=="Windows") Windows <- TRUE
    url.data.file <- "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
    temp.zip.file.name <- "temp.zip"
    if(Windows) download.file(url.data.file, temp.zip.file.name) else
        download.file(url.data.file, temp.zip.file.name, method="curl")
    unzip(temp.zip.file.name)
    file.remove(temp.zip.file.name)
}
```

``` {r echo = TRUE}
# Here we create a Corpus.
crp <- VCorpus(DirSource("final/en_US/", encoding = "UTF-8"),                               readerControl = list(language = "en_US", load = T))

```

Let us have a quick initial look on what we get.

``` {r echo = FALSE}
q.summ <- data.frame(matrix(0, ncol = 4, nrow = 3))
q.summ[, 1] <- unlist(sapply(paste("final/en_US/", list.files("final/en_US/"), sep = ""), file.info)[1,])/1024/1024
q.summ[, 2] <- sapply(names(crp), function(x) length(crp[[x]][[1]])/1000)
q.summ[, 3] <- sapply(names(crp), 
                      function(x) sum(stri_count(crp[[x]][[1]],regex="\\S+"))/1000000)
q.summ[, 4] <- sapply(names(crp), function(x) sum(nchar(crp[[x]][[1]]))/1000000)
q.summ <- round(q.summ, 0)
names(q.summ) <- c("File size (Mb)", "Lines (thds)", "Word (mln)", "Characters (mln)")
row.names(q.summ) <- c("Blogs", "News", "Twits")
q.summ
rm(crp) #Removing corpus, because it is too big.
```

It looks like we have enough data for prediction. It makes sence that before exploratory analysis we do some sampling to spend resonable time for calculations. Let us take only **n** lines fron each of three provided files (blogs, new, twitter).
``` {r echo = TRUE}
n <- 1000
set.seed(8310)
blogs <- readLines("final/en_US/en_US.blogs.txt", skipNul = T)
blogs <- sample(blogs, n)
news <- readLines("final/en_US/en_US.news.txt", skipNul = T)
news <- sample(news, n)
twitter <- readLines("final/en_US/en_US.twitter.txt", skipNul = T)
twitter <- sample(twitter, n)

```

###Preliminary considerations
Before creation a model we have to clean data. The are several standard steps in linguistic data mining, which are putting all letters to lower case, removing extra spaces, punctuation and other non-alphabetic symbols, removing profanity words and so-called "stop-word", working with roots of words (tokenization, stemming or lemmatization) and creating n-grams. Here are some considerations regarding some steps in the light of our task.

1. Since we have texts from twitter we have to remove all words in the form of **@user** and **#hashtag** before removing all the non-alphabetic symbols, otherwise we will have many senseless words.
2. Should we remove apostrophes? They indicate possessive mode in English, can they change meaning of the frase? That is the question we can answer at the stage of building models.
3. Should we remove dashes? They create new composite word, like **Alabama-based** which obviously have to be predicted separately. However since we are going to use 2- and 3-grams, it looks like dashes can be removed. 
4. It looks like we also have to mark boundaries of sentences, they seem as inportant information for prediction. At least that will help us to predict the first word if user will not insert anything into line.
5. For tokenized text words "car" and "cars" are different. For stemmed text words "car" and "cars" are the same. For lemmatisated text words "car", "cars", "automobile" and "automobiles" are the same. At first glance we have to tokenize our text for better prediction. However this is the topic of following researches which are beyond exploratory analysis.
6. We also want to remove all profanity words because we do not want to predict them. However "stop word" like **the**, **a**, **for**, **nor**, **but** etc. we would like to leave because we are going to predict them too.

###Research of sentences

So, first of all, let us break our text in sentences and insert special marks for start of sentence (_sofs_) and end of sentence (_eofs_). Obviously we have to do this before cleaning punctuations. Later we will try to make preliminary conclusion should we consider boundaries of sentences in our model. We show calculations only for blogs, other sets if text give same results.
``` {r echo = F}
#Function marks boundaries of sentences
convert.text.to.sentences <- function(text) {
  
    sentence_token_annotator <- Maxent_Sent_Token_Annotator()
    text <- as.String(text) #Have to do this to use Annotator
    sentence.boundaries <- annotate(text, sentence_token_annotator)
    sentences <- text[sentence.boundaries] #Now all lines has one sentence
    
    sentences <- paste("sofs", sentences, "eofs", sep=" ")
    
    sentences
}
```

```{r echo = TRUE}
blogs.sent <- convert.text.to.sentences(blogs)
```

Now we can clean both initial texs and text with boundaries of sentences.

``` {r echo = F}

clean_twitter <- function(x){ #Function removes all #hashtag and @users in a line
  x <- gsub("[@#]\\S+\\w", "", x)
  x
}

#Function cleans the x using profanity from file
clean_text <- function(x, profanity) {
  
  bad.words <- readLines(profanity)
  
  x <- clean_twitter(x)
  x <- stripWhitespace(x)
  x <- tolower(x)
  x <- removePunctuation(x)
  x <- removeNumbers(x) 
  x <- removeWords(x, bad.words)
}
```


``` {r echo = TRUE}
blogs <- clean_text(blogs, "profanity.txt")
blogs.sent <- clean_text(blogs.sent, "profanity.txt")
```

And now we tokenize both text with sentence's boundaries and text without them. Than we build 2-grams for first one and 1-gram for second one to estimate the differencies and decide shoud we include this information into our model. 
``` {r echo = FALSE}
OnegramTokenizer <-
  function(x)
    unlist(lapply(ngrams(words(x), 1), paste, collapse = " "), use.names = FALSE)

BigramTokenizer <-
  function(x)
    unlist(lapply(ngrams(words(x), 2), paste, collapse = " "), use.names = FALSE)

TrigramTokenizer <-
  function(x)
    unlist(lapply(ngrams(words(x), 3), paste, collapse = " "), use.names = FALSE)
```

``` {r echo = TRUE}
blogs <- paste(blogs, collapse = " ")
blogs.sent <- paste(blogs.sent, collapse = " ")
crp.blogs <- VCorpus(VectorSource(blogs))
crp.blogs.sent <- VCorpus(VectorSource(blogs.sent))
blogs.matrix <- TermDocumentMatrix(crp.blogs, 
                                   control = list(tokenize = OnegramTokenizer))
sent.matrix <- TermDocumentMatrix(crp.blogs.sent, 
                                  control = list(tokenize = BigramTokenizer))
```

``` {r echo = FALSE}
converse.TDM <- function(x){
  x <- as.data.frame(as.matrix(x))
  x$TOTAL <- rowSums(x)
  x <- x[order(-x$TOTAL),]
  x <- data.frame(row.names(x), x$TOTAL)
  names(x) <- c("Words", "Frequency")
  x
}

plotting.ngrams <- function(df, n=6){
  df <- converse.TDM(df)
  df <- df[1:n,]
  p <-  ggplot(df, aes(x=reorder(Words, Frequency), Frequency, 
                      ymax = max(Frequency)*1.05))
  p <- p + geom_bar(stat = "identity", color = "lightgreen", fill = "lightgreen") + 
           geom_text(aes(label = Frequency), size = 3, 
                     position = "stack", hjust = 1) +
           xlab("Words") + 
           ylab("Frequency") + 
           ggtitle(paste(n, "most frequent n-grams")) + 
           coord_flip()
  p
}
```

``` {r echo = T}
blogs.stat <- converse.TDM(blogs.matrix)
blogs.sent.stat <- converse.TDM(sent.matrix)
blogs.sent.stat <- blogs.sent.stat[-which(blogs.sent.stat$Words == "eofs sofs"),]
blogs.sent.stat[-grep("sofs", blogs.sent.stat$Words),]
cbind(head(blogs.sent.stat, 10), head(blogs.stat, 10))
```

###Conclusion
As we can see most popular word at the start of the sentence is different from the most poular word in the middle of the text. We can use this knowledge for more accurate prediction, when for example a user just open the program but still did not type anything. Also we can use knowledge about and of sentence to predict points and commas.

##Exploratory Analysis

Now let's have a look on the all three data set and find the answers for questions. But first let us clean them and do corpus of those texts as we did with blogs (code not shown).

``` {r echo = T}

news <- clean_text(news, "profanity.txt")
twitter <- clean_text(twitter, "profanity.txt")
news <- paste(news, collapse = " ")
twitter <- paste(twitter, collapse = " ")
```

``` {r echo = T}
docs <- c(blogs, news, twitter)
whole.corpus <- VCorpus(VectorSource(docs))
```

###Some words are more frequent than others - what are the distributions of word frequencies? 
``` {r echo = T}
one.matrix <- TermDocumentMatrix(whole.corpus, 
                                 control = list(tokenize = OnegramTokenizer))
bi.matrix <- TermDocumentMatrix(whole.corpus, 
                                 control = list(tokenize = BigramTokenizer))
tri.matrix <- TermDocumentMatrix(whole.corpus, 
                                 control = list(tokenize = TrigramTokenizer))

plotting.ngrams(one.matrix, 20)
```

As we can see, most frequent word are so called "stopwords". We can not get rid of them since we have to predict them too. But that would be interesting to have a look what is the frequency of word in corpus cleaned from "stopword". Let us have a look on cleaned corpus (code not shown).

``` {r echo = FALSE}
#Here we create NoStopWord corpus and all n-gram for it
nsw.crp<-tm_map(whole.corpus,removeWords,stopwords('en'))
nsw.one.matrix <- TermDocumentMatrix(nsw.crp, 
                                     control = list(tokenize = OnegramTokenizer))

plotting.ngrams(nsw.one.matrix, 20)
```

We can see, there are no big differencies in frequencies of words comparing to "stopwords".


###What are the frequencies of 2-grams and 3-grams in the dataset? 
``` {r echo = T}
plotting.ngrams(bi.matrix, 20)
plotting.ngrams(tri.matrix, 20)
```


###How many unique words do you need in a frequency sorted dictionary to cover 50% of all word instances in the language? 90%?
``` {r echo = FALSE}
coverage <- function(df, limit = 0.5){
    df <- converse.TDM(df)
    sum <- 0
    total <- sum(df$Frequency)
    n <- length(df$Frequency)
    j <-0
    for(i in 1:n){
        summ <- sum(df$Frequency[1:i])
        if (summ >= limit*total) {j <- i; break}
    }
    j
}
```

We have **`r dim(converse.TDM(one.matrix))[1]`** unique words in our corpus with "stopwords" and **`r dim(converse.TDM(nsw.one.matrix))[1]`** in corpus without "stopwords". To cover 50% of frequency we need only **`r coverage(one.matrix)`** words with stopwords and **`r coverage(nsw.one.matrix)`** words without stopwords. In case of 90% coverage the numbers are **`r coverage(one.matrix, 0.9)`** and **`r coverage(nsw.one.matrix, 0.9)`** respectively.

###How do you evaluate how many of the words come from foreign languages? 

If we had a trusted English dictionary, we could remove all the listed words from corpus as we do with "stopwords" or profanity words. Everything left would shows us all foreign words.

###Can you think of a way to increase the coverage -- identifying words that may not be in the corpora or using a smaller number of words in the dictionary to cover the same number of phrases?

We think both ways are possible. We can use smaller amount of words to increase the coverage if we know the context of corpus. Or we can consider synonims to cover more meaning with less words. Second way is more simplier. It looks like lemmatization can help.

##Appendix - Functions used in the paper.
``` {r echo = T}

#Function marks boundaries of sentences
convert.tex.to.sentences <- function(text) {
  
  sentence_token_annotator <- Maxent_Sent_Token_Annotator()
  text <- as.String(text) #Have to do this to use Annotator
  sentence.boundaries <- annotate(text, sentence_token_annotator)
  sentences <- text[sentence.boundaries] #Now all lines has one sentence
  sentences <- paste("sofs", sentences, "eofs", sep=" ")
  sentences
}

#Function removes all #hashtag and @users in a line
clean_twitter <- function(x){
  x <- gsub("[@#]\\S+\\w", "", x)
  x
}

#Function cleans the x using profanity from file
clean_text <- function(x, profanity) {
  
  bad.words <- readLines(profanity)
  
  x <- clean_twitter(x)
  x <- stripWhitespace(x)
  x <- tolower(x)
  x <- removePunctuation(x)
  x <- removeNumbers(x) 
  x <- removeWords(x, bad.words)
}

#Functions build 1-, 2- anf 3-grams
OnegramTokenizer <-
  function(x)
    unlist(lapply(ngrams(words(x), 1), paste, collapse = " "), use.names = FALSE)

BigramTokenizer <-
  function(x)
    unlist(lapply(ngrams(words(x), 2), paste, collapse = " "), use.names = FALSE)

TrigramTokenizer <-
  function(x)
    unlist(lapply(ngrams(words(x), 3), paste, collapse = " "), use.names = FALSE)

#Function to convert TextDocumentMatrix into data frame
converse.TDM <- function(x){
  x <- as.data.frame(as.matrix(x))
  x$TOTAL <- rowSums(x)
  x <- x[order(-x$TOTAL),]
  x <- data.frame(row.names(x), x$TOTAL)
  names(x) <- c("Words", "Frequency")
  x
}

#Functions just build a plot from Text Document Matrix
plotting.ngrams <- function(df, n=6){
  df <- converse.TDM(df)
  df <- df[1:n,]
  p <-  ggplot(df, aes(x=reorder(Words, Frequency), Frequency, 
                      ymax = max(Frequency)*1.05))
  p <- p + geom_bar(stat = "identity", color = "lightgreen", fill = "lightgreen") + 
           geom_text(aes(label = Frequency), size = 3, 
                     position = "stack", hjust = 1) +
           xlab("Words") + 
           ylab("Frequency") + 
           ggtitle(paste(n, "most frequent n-grams")) + 
           coord_flip()
  p
}

#Function calculate how many unique words do you need to cover 
#given percentage
coverage <- function(df, limit = 0.5){
    df <- converse.TDM(df)
    sum <- 0
    total <- sum(df$Frequency)
    n <- length(df$Frequency)
    j <-0
    for(i in 1:n){
        summ <- sum(df$Frequency[1:i])
        if (summ >= limit*total) {j <- i; break}
    }
    j
}

```

